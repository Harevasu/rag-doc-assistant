# RAG Document Assistant

An AI-powered Document Assistant that uses **Retrieval-Augmented Generation (RAG)** to perform semantic search and answer user questions from PDFs and text documents.

![RAG Architecture](https://img.shields.io/badge/RAG-Retrieval%20Augmented%20Generation-blue)
![Python](https://img.shields.io/badge/Python-3.9+-green)
![FastAPI](https://img.shields.io/badge/FastAPI-0.109+-red)
![License](https://img.shields.io/badge/License-MIT-yellow)

## âœ¨ Features

- ğŸ“„ **Document Upload**: Support for PDF, TXT, and Markdown files
- ğŸ” **Semantic Search**: Find relevant content using vector embeddings
- ğŸ¤– **AI-Powered Q&A**: Get accurate answers with source citations
- ğŸ’¾ **Persistent Storage**: ChromaDB for reliable vector storage
- ğŸ¨ **Modern UI**: Beautiful dark theme with chat interface
- ğŸ”„ **Multiple LLM Providers**: Support for Ollama (local), Gemini API, and fine-tuned models
- âš¡ **Offline Mode**: Run completely offline with Ollama or fine-tuned models

## ğŸ—ï¸ Architecture

```
User Question â†’ Embedding â†’ Vector Search â†’ Context Retrieval â†’ LLM â†’ Answer
```

```mermaid
graph LR
    A[User Query] --> B[Embedding Model]
    B --> C[Vector Search]
    C --> D[ChromaDB]
    D --> E[Context Retrieval]
    E --> F[LLM Provider]
    F --> G[Generated Answer]
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.9+
- [Ollama](https://ollama.ai/) (for local LLM) **OR** Google API Key (for Gemini)

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/rag-doc-assistant.git
cd rag-doc-assistant

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install Python dependencies
pip install -r backend/requirements.txt
```

### Configuration

Copy the example environment file and configure:

```bash
cp .env.example backend/.env
```

Edit `backend/.env` with your settings:

```env
# LLM Provider: "finetuned" | "ollama" | "gemini"
LLM_PROVIDER=ollama

# Ollama Configuration (for local LLM)
OLLAMA_HOST=http://localhost:11434
OLLAMA_LLM_MODEL=llama3.2
OLLAMA_EMBED_MODEL=nomic-embed-text

# Gemini Configuration (for API-based LLM)
GOOGLE_API_KEY=your_api_key_here
EMBEDDING_MODEL=models/text-embedding-004
LLM_MODEL=gemini-2.0-flash

# Chunking Configuration
CHUNK_SIZE=512
CHUNK_OVERLAP=50
```

### Running the Application

**1. Start Ollama (if using local LLM):**

```bash
ollama serve
ollama pull llama3.2
ollama pull nomic-embed-text
```

**2. Start the backend server:**

```bash
cd backend
uvicorn main:app --reload --port 8000
```

**3. Open the frontend:**

Open `frontend/index.html` in your browser, or serve it:

```bash
cd frontend
python -m http.server 3000
```

Visit: http://localhost:3000

## ğŸ“š API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | Health check |
| `/status` | GET | System status & document count |
| `/upload` | POST | Upload and index a document |
| `/query` | POST | Ask a question (RAG) |
| `/documents` | GET | List all indexed documents |
| `/documents/{id}` | DELETE | Remove a document |

## ğŸ“ Project Structure

```
rag-doc-assistant/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py              # FastAPI entry point
â”‚   â”œâ”€â”€ config.py            # Configuration management
â”‚   â”œâ”€â”€ document_processor.py # PDF/text extraction & chunking
â”‚   â”œâ”€â”€ embeddings.py        # Embedding generation (Ollama/Gemini)
â”‚   â”œâ”€â”€ vector_store.py      # ChromaDB operations
â”‚   â”œâ”€â”€ rag_pipeline.py      # RAG orchestration & LLM providers
â”‚   â”œâ”€â”€ trainer.py           # Fine-tuning utilities
â”‚   â””â”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html           # Main UI
â”‚   â”œâ”€â”€ styles.css           # Styling
â”‚   â””â”€â”€ app.js               # Frontend logic
â”œâ”€â”€ uploads/                  # Uploaded documents (gitignored)
â”œâ”€â”€ data/                     # ChromaDB storage (gitignored)
â”œâ”€â”€ .env.example             # Environment template
â””â”€â”€ README.md
```

## ğŸ’¡ Usage

1. **Upload Documents**: Drag & drop or click to upload PDF/TXT/MD/docs files
2. **Select LLM Provider**: Choose between Ollama, Gemini, or fine-tuned model
3. **Ask Questions**: Type your question in the chat input
4. **Get Answers**: Receive AI-generated answers with source citations
5. **Manage Documents**: View and delete indexed documents in the sidebar

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|------------|
| **Backend** | Python, FastAPI, Uvicorn |
| **Vector DB** | ChromaDB |
| **Embeddings** | Ollama (nomic-embed-text) / Gemini |
| **LLM** | Ollama (llama3.2) / Gemini / Fine-tuned Flan-T5 |
| **PDF Parser** | PyMuPDF |
| **Frontend** | HTML, CSS, JavaScript |

## ğŸ”§ LLM Providers

### Ollama (Recommended for Offline)
- Completely free and runs locally
- Requires [Ollama](https://ollama.ai/) installation
- Models: `llama3.2`, `nomic-embed-text`

### Gemini API
- Fast and powerful cloud-based LLM
- Requires Google API key
- Free tier available with quota limits

### Fine-tuned Model
- Custom model trained on your documents
- Best for domain-specific Q&A
- Run the trainer to generate your model




---

Built with â¤ï¸ using RAG Architecture
